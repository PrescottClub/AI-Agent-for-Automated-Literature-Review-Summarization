"""
FastAPI æœåŠ¡å™¨ - ä¸º Vue3 å‰ç«¯æä¾› API æ¥å£
"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import time

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
src_dir = current_dir / "src"
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

try:
    from lit_review_agent.agent import LiteratureAgent
    from lit_review_agent.utils.config import Config
except ImportError as e:
    print(f"Warning: Could not import LiteratureAgent: {e}")
    LiteratureAgent = None
    Config = None

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="AI Literature Review API",
    description="æ™ºèƒ½æ–‡çŒ®ç»¼è¿°ç³»ç»Ÿ API",
    version="1.0.0"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vue3 å¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class SearchRequest(BaseModel):
    query: str
    sources: List[str] = ["arxiv", "semantic_scholar"]
    maxPapers: int = 20
    yearStart: Optional[int] = None
    yearEnd: Optional[int] = None
    retrieveFullText: bool = False
    enableAIAnalysis: bool = True

class ReportRequest(BaseModel):
    papers: List[Dict]
    title: str

# å“åº”æ¨¡å‹
class Paper(BaseModel):
    title: str
    authors: List[str]
    publishedDate: str
    source: str
    summary: str
    keywords: Optional[List[str]] = []
    url: Optional[str] = None
    pdfUrl: Optional[str] = None
    fullTextRetrieved: Optional[bool] = False

class SearchResult(BaseModel):
    papers: List[Paper]
    totalCount: int
    processingTime: float
    summary: Optional[str] = None

# å…¨å±€å˜é‡
literature_agent = None

def get_agent():
    """è·å–æ–‡çŒ®ä»£ç†å®ä¾‹"""
    global literature_agent
    if literature_agent is None and LiteratureAgent and Config:
        try:
            config = Config()
            literature_agent = LiteratureAgent(config)
        except Exception as e:
            print(f"Failed to initialize LiteratureAgent: {e}")
            literature_agent = None
    return literature_agent

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    print("ğŸš€ å¯åŠ¨ AI Literature Review API æœåŠ¡å™¨...")
    agent = get_agent()
    if agent:
        print("âœ… æ–‡çŒ®ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
    else:
        print("âŒ æ–‡çŒ®ä»£ç†åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "AI Literature Review API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/api/status")
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    agent = get_agent()
    return {
        "status": "healthy" if agent else "demo",
        "timestamp": datetime.now().isoformat(),
        "agent_initialized": agent is not None
    }

@app.post("/api/search", response_model=SearchResult)
async def search_literature(request: SearchRequest):
    """æ–‡çŒ®æ£€ç´¢"""
    start_time = time.time()
    
    try:
        agent = get_agent()
        
        print(f"ğŸ” å¼€å§‹æ£€ç´¢: {request.query}")
        
        if agent:
            # ä½¿ç”¨çœŸå®çš„ä»£ç†
            search_params = {
                "query": request.query,
                "max_papers": request.maxPapers,
                "sources": request.sources,
                "full_text": request.retrieveFullText,
            }
            
            if request.yearStart:
                search_params["year_start"] = request.yearStart
            if request.yearEnd:
                search_params["year_end"] = request.yearEnd
            
            results = await asyncio.to_thread(agent.conduct_review, **search_params)
            
            # è½¬æ¢ç»“æœæ ¼å¼
            papers = []
            if results and "processed_papers" in results:
                for paper_data in results["processed_papers"]:
                    paper = Paper(
                        title=paper_data.get("title", "æœªçŸ¥æ ‡é¢˜"),
                        authors=paper_data.get("authors", []),
                        publishedDate=paper_data.get("published_date", ""),
                        source=paper_data.get("source", "unknown"),
                        summary=paper_data.get("ai_enhanced_summary", paper_data.get("original_summary", "")),
                        keywords=paper_data.get("keywords", []),
                        url=paper_data.get("url", ""),
                        pdfUrl=paper_data.get("pdf_url", ""),
                        fullTextRetrieved=paper_data.get("full_text_retrieved", False)
                    )
                    papers.append(paper)
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            papers = [
                Paper(
                    title=f"äººå·¥æ™ºèƒ½åœ¨{request.query}é¢†åŸŸçš„åº”ç”¨ç ”ç©¶",
                    authors=["å¼ ä¸‰", "æå››", "ç‹äº”"],
                    publishedDate="2024-01-15",
                    source="arxiv",
                    summary=f"æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨{request.query}é¢†åŸŸçš„æœ€æ–°åº”ç”¨ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰å‰æ²¿æŠ€æœ¯çš„å®é™…åº”ç”¨æ¡ˆä¾‹ã€‚",
                    keywords=["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", request.query],
                    url="https://arxiv.org/abs/2401.12345",
                    fullTextRetrieved=True
                ),
                Paper(
                    title=f"{request.query}ä¸­çš„æœºå™¨å­¦ä¹ æ–¹æ³•ç»¼è¿°",
                    authors=["èµµå…­", "é’±ä¸ƒ"],
                    publishedDate="2023-12-20",
                    source="semantic_scholar",
                    summary=f"æœ¬ç»¼è¿°åˆ†æäº†{request.query}é¢†åŸŸä¸­æœºå™¨å­¦ä¹ æ–¹æ³•çš„å‘å±•ç°çŠ¶ã€ä¸»è¦æŒ‘æˆ˜å’Œæœªæ¥è¶‹åŠ¿ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
                    keywords=["æœºå™¨å­¦ä¹ ", "æ•°æ®åˆ†æ", "ç®—æ³•ä¼˜åŒ–"],
                    url="https://example.com/paper2",
                    fullTextRetrieved=False
                ),
                Paper(
                    title=f"åŸºäºæ·±åº¦å­¦ä¹ çš„{request.query}æ™ºèƒ½ç³»ç»Ÿè®¾è®¡",
                    authors=["å­™å…«", "å‘¨ä¹", "å´å"],
                    publishedDate="2024-02-10",
                    source="arxiv",
                    summary=f"æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„{request.query}æ™ºèƒ½ç³»ç»Ÿæ¶æ„ï¼Œé€šè¿‡å®éªŒéªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚",
                    keywords=["æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½ç³»ç»Ÿ", "ç³»ç»Ÿæ¶æ„"],
                    url="https://arxiv.org/abs/2402.67890",
                    fullTextRetrieved=True
                )
            ]
        
        processing_time = time.time() - start_time
        
        print(f"âœ… æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œè€—æ—¶ {processing_time:.2f}s")
        
        return SearchResult(
            papers=papers,
            totalCount=len(papers),
            processingTime=processing_time,
            summary=f"åŸºäº'{request.query}'çš„æ–‡çŒ®æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ°{len(papers)}ç¯‡ç›¸å…³è®ºæ–‡ã€‚"
        )
        
    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ£€ç´¢å¤±è´¥: {str(e)}")

@app.post("/api/generate-report")
async def generate_report(request: ReportRequest):
    """ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
    try:
        print(f"ğŸ“ å¼€å§‹ç”ŸæˆæŠ¥å‘Š: {request.title}")
        
        # æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆ
        await asyncio.sleep(2)
        
        report = f"""# {request.title}

## æ‘˜è¦

æœ¬æŠ¥å‘ŠåŸºäº {len(request.papers)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œå¯¹ç ”ç©¶ä¸»é¢˜è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°åˆ†æã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„æ–‡çŒ®è°ƒç ”å’Œæ·±å…¥åˆ†æï¼Œæœ¬æŠ¥å‘Šæ€»ç»“äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶ã€ä¸»è¦æˆæœå’Œå‘å±•è¶‹åŠ¿ã€‚

## ç ”ç©¶èƒŒæ™¯

éšç€ç§‘æŠ€çš„å¿«é€Ÿå‘å±•ï¼Œè¯¥ç ”ç©¶é¢†åŸŸæ­£åœ¨ç»å†å‰æ‰€æœªæœ‰çš„å˜é©ã€‚æœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›è¯¥é¢†åŸŸçš„å…¨é¢è§†è§’ã€‚

## ä¸»è¦å‘ç°

### 1. ç ”ç©¶ç°çŠ¶
- å…±æ£€ç´¢åˆ° {len(request.papers)} ç¯‡é«˜è´¨é‡ç›¸å…³æ–‡çŒ®
- ç ”ç©¶æ¶µç›–äº†å¤šä¸ªé‡è¦çš„æŠ€æœ¯æ–¹å‘å’Œåº”ç”¨åœºæ™¯
- è¿‘å¹´æ¥è¯¥é¢†åŸŸçš„ç ”ç©¶çƒ­åº¦æŒç»­ä¸Šå‡

### 2. æŠ€æœ¯è¶‹åŠ¿
- äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨
- è·¨å­¦ç§‘èåˆæˆä¸ºé‡è¦å‘å±•æ–¹å‘
- å®é™…åº”ç”¨åœºæ™¯ä¸æ–­æ‰©å±•

### 3. ä¸»è¦æŒ‘æˆ˜
- æŠ€æœ¯æ ‡å‡†åŒ–ç¨‹åº¦æœ‰å¾…æé«˜
- æ•°æ®è´¨é‡å’Œéšç§ä¿æŠ¤éœ€è¦å¹³è¡¡
- äº§ä¸šåŒ–åº”ç”¨ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜

## æœªæ¥å±•æœ›

åŸºäºå½“å‰çš„ç ”ç©¶è¶‹åŠ¿å’ŒæŠ€æœ¯å‘å±•ï¼Œæˆ‘ä»¬é¢„æµ‹è¯¥é¢†åŸŸåœ¨æœªæ¥å‡ å¹´å°†ç»§ç»­å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

1. **æŠ€æœ¯åˆ›æ–°**: æ–°ç®—æ³•å’Œæ–°æ–¹æ³•çš„ä¸æ–­æ¶Œç°
2. **åº”ç”¨æ‹“å±•**: æ›´å¤šå®é™…åº”ç”¨åœºæ™¯çš„æ¢ç´¢
3. **æ ‡å‡†åŒ–**: è¡Œä¸šæ ‡å‡†å’Œè§„èŒƒçš„é€æ­¥å»ºç«‹

## ç»“è®º

é€šè¿‡å¯¹ç°æœ‰æ–‡çŒ®çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å‘ç°è¯¥é¢†åŸŸæ­£å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µï¼Œå…·æœ‰å¹¿é˜”çš„ç ”ç©¶å‰æ™¯å’Œåº”ç”¨æ½œåŠ›ã€‚å»ºè®®ç ”ç©¶äººå‘˜å…³æ³¨æŠ€æœ¯åˆ›æ–°ä¸å®é™…åº”ç”¨çš„ç»“åˆï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„æŒç»­å¥åº·å‘å±•ã€‚

## å‚è€ƒæ–‡çŒ®

æœ¬æŠ¥å‘ŠåŸºäº {len(request.papers)} ç¯‡é«˜è´¨é‡å­¦æœ¯è®ºæ–‡ï¼Œæ¶µç›–äº†è¯¥é¢†åŸŸçš„ä¸»è¦ç ”ç©¶æˆæœå’Œæœ€æ–°è¿›å±•ã€‚

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*
*ç”Ÿæˆå·¥å…·: AI Literature Review System*
        """
        
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        return {"report": report}
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ å¯åŠ¨ FastAPI æœåŠ¡å™¨...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    ) 