#!/usr/bin/env python3
"""
FastAPI æœåŠ¡å™¨ - ä¸º Vue3 å‰ç«¯æä¾› API æ¥å£
"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import time

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent.parent.parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

try:
    from src.lit_review_agent.agent import LiteratureAgent
    from src.lit_review_agent.utils.config import Config
except ImportError as e:
    print(f"Warning: Could not import LiteratureAgent: {e}")
    LiteratureAgent = None
    Config = None

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="AI Literature Review API",
    description="æ™ºèƒ½æ–‡çŒ®ç»¼è¿°ç³»ç»Ÿ API",
    version="1.0.0"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vue3 å¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class SearchRequest(BaseModel):
    query: str
    sources: List[str] = ["arxiv", "semantic_scholar"]
    maxPapers: int = 20
    yearStart: Optional[int] = None
    yearEnd: Optional[int] = None
    retrieveFullText: bool = False
    enableAIAnalysis: bool = True

class ReportRequest(BaseModel):
    papers: List[Dict]
    title: str

# å“åº”æ¨¡å‹
class Paper(BaseModel):
    title: str
    authors: List[str]
    publishedDate: str
    source: str
    summary: str
    keywords: Optional[List[str]] = []
    url: Optional[str] = None
    pdfUrl: Optional[str] = None
    fullTextRetrieved: Optional[bool] = False

class SearchResult(BaseModel):
    papers: List[Paper]
    totalCount: int
    processingTime: float
    summary: Optional[str] = None

# å…¨å±€å˜é‡
literature_agent = None

def get_agent():
    """è·å–æ–‡çŒ®ä»£ç†å®ä¾‹"""
    global literature_agent
    if literature_agent is None and LiteratureAgent and Config:
        try:
            config = Config()
            literature_agent = LiteratureAgent(config)
        except Exception as e:
            print(f"Failed to initialize LiteratureAgent: {e}")
            literature_agent = None
    return literature_agent

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    print("ğŸš€ å¯åŠ¨ AI Literature Review API æœåŠ¡å™¨...")
    agent = get_agent()
    if agent:
        print("âœ… æ–‡çŒ®ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
    else:
        print("âŒ æ–‡çŒ®ä»£ç†åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "AI Literature Review API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/api/status")
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    agent = get_agent()
    return {
        "status": "healthy" if agent else "demo",
        "timestamp": datetime.now().isoformat(),
        "agent_initialized": agent is not None
    }

@app.post("/api/search", response_model=SearchResult)
async def search_literature(request: SearchRequest):
    """æ–‡çŒ®æ£€ç´¢"""
    start_time = time.time()
    
    try:
        agent = get_agent()
        
        print(f"ğŸ” å¼€å§‹æ£€ç´¢: {request.query}")
        
        if agent:
            # ä½¿ç”¨çœŸå®çš„ä»£ç†
            results = await agent.conduct_literature_review(
                research_topic=request.query,
                max_papers=request.maxPapers,
                retrieve_full_text=request.retrieveFullText
            )
            
            # è½¬æ¢ç»“æœæ ¼å¼
            papers = []
            if results and "papers" in results:
                for paper_data in results["papers"]:
                    paper = Paper(
                        title=paper_data.get("title", "æœªçŸ¥æ ‡é¢˜"),
                        authors=paper_data.get("authors", []),
                        publishedDate=paper_data.get("published_date", ""),
                        source=paper_data.get("source", "unknown"),
                        summary=paper_data.get("ai_enhanced_summary", paper_data.get("summary", "")),
                        keywords=paper_data.get("keywords", []),
                        url=paper_data.get("url", ""),
                        pdfUrl=paper_data.get("pdf_url", ""),
                        fullTextRetrieved=paper_data.get("full_text_retrieved", False)
                    )
                    papers.append(paper)
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            papers = [
                Paper(
                    title=f"äººå·¥æ™ºèƒ½åœ¨{request.query}é¢†åŸŸçš„åº”ç”¨ç ”ç©¶",
                    authors=["å¼ ä¸‰", "æå››", "ç‹äº”"],
                    publishedDate="2024-01-15",
                    source="arxiv",
                    summary=f"æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨{request.query}é¢†åŸŸçš„æœ€æ–°åº”ç”¨ã€‚",
                    keywords=["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", request.query],
                    url="https://arxiv.org/abs/2401.12345",
                    fullTextRetrieved=True
                ),
                Paper(
                    title=f"{request.query}ä¸­çš„æœºå™¨å­¦ä¹ æ–¹æ³•ç»¼è¿°",
                    authors=["èµµå…­", "é’±ä¸ƒ"],
                    publishedDate="2023-12-20",
                    source="semantic_scholar",
                    summary=f"æœ¬ç»¼è¿°åˆ†æäº†{request.query}é¢†åŸŸä¸­æœºå™¨å­¦ä¹ æ–¹æ³•çš„å‘å±•ç°çŠ¶ã€‚",
                    keywords=["æœºå™¨å­¦ä¹ ", "æ•°æ®åˆ†æ"],
                    url="https://example.com/paper2",
                    fullTextRetrieved=False
                )
            ]
        
        processing_time = time.time() - start_time
        
        print(f"âœ… æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œè€—æ—¶ {processing_time:.2f}s")
        
        return SearchResult(
            papers=papers,
            totalCount=len(papers),
            processingTime=processing_time,
            summary=f"åŸºäº'{request.query}'çš„æ–‡çŒ®æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ°{len(papers)}ç¯‡ç›¸å…³è®ºæ–‡ã€‚"
        )
        
    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ£€ç´¢å¤±è´¥: {str(e)}")

@app.post("/api/generate-report")
async def generate_report(request: ReportRequest):
    """ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
    try:
        print(f"ğŸ“ å¼€å§‹ç”ŸæˆæŠ¥å‘Š: {request.title}")
        
        # æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆ
        await asyncio.sleep(2)
        
        report = f"""# {request.title}

## æ‘˜è¦

æœ¬æŠ¥å‘ŠåŸºäº {len(request.papers)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œå¯¹ç ”ç©¶ä¸»é¢˜è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°åˆ†æã€‚

## ä¸»è¦å‘ç°

### 1. ç ”ç©¶ç°çŠ¶
- å…±æ£€ç´¢åˆ° {len(request.papers)} ç¯‡é«˜è´¨é‡ç›¸å…³æ–‡çŒ®
- ç ”ç©¶æ¶µç›–äº†å¤šä¸ªé‡è¦çš„æŠ€æœ¯æ–¹å‘å’Œåº”ç”¨åœºæ™¯

### 2. æŠ€æœ¯è¶‹åŠ¿
- äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨
- è·¨å­¦ç§‘èåˆæˆä¸ºé‡è¦å‘å±•æ–¹å‘

## ç»“è®º

é€šè¿‡å¯¹ç°æœ‰æ–‡çŒ®çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å‘ç°è¯¥é¢†åŸŸæ­£å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µã€‚

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*
        """
        
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        return {"report": report}
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ å¯åŠ¨ FastAPI æœåŠ¡å™¨...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )
