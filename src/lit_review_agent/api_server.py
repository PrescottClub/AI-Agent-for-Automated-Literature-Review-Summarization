#!/usr/bin/env python3
"""
FastAPI æœåŠ¡å™¨ - ä¸º Vue3 å‰ç«¯æä¾› API æ¥å£
"""

import asyncio
import sys
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import time

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent.parent.parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

try:
    from src.lit_review_agent.agent import LiteratureAgent
    from src.lit_review_agent.utils.config import Config
except ImportError as e:
    print(f"Warning: Could not import modules: {e}")
    LiteratureAgent = None
    Config = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    print(">> å¯åŠ¨ AI Literature Review API æœåŠ¡å™¨...")
    try:
        agent = get_agent()
        if agent:
            print(">> æ–‡çŒ®ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
        else:
            print(">> æ–‡çŒ®ä»£ç†åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    except Exception as e:
        print(f">> ä»£ç†åˆå§‹åŒ–å¼‚å¸¸: {e}")
        print(">> å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼è¿è¡Œ")

    yield

    # å…³é—­æ—¶æ¸…ç†
    print(">> å…³é—­ AI Literature Review API æœåŠ¡å™¨...")
    global literature_agent
    if literature_agent:
        try:
            # æ¸…ç†èµ„æº
            literature_agent = None
            print(">> èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f">> èµ„æºæ¸…ç†å¼‚å¸¸: {e}")


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="AI Literature Review API",
    description="æ™ºèƒ½æ–‡çŒ®ç»¼è¿°ç³»ç»Ÿ API",
    version="3.1.0",
    lifespan=lifespan,
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:3000",
        "http://127.0.0.1:5173",
        "http://127.0.0.1:3000",
        "file://",
        "*"  # å¼€å‘ç¯å¢ƒå…è®¸æ‰€æœ‰æº
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ç®€åŒ–ç‰ˆæœ¬ - ç§»é™¤å¤æ‚ä¸­é—´ä»¶


# è¯·æ±‚æ¨¡å‹
class SearchRequest(BaseModel):
    query: Optional[str] = None  # Legacy structured query field
    rawQuery: Optional[str] = None  # New natural language query field
    sources: List[str] = ["arxiv", "semantic_scholar"]
    maxPapers: int = 20
    yearStart: Optional[int] = None
    yearEnd: Optional[int] = None
    retrieveFullText: bool = False
    enableAIAnalysis: bool = True


class ReportRequest(BaseModel):
    papers: List[Dict]
    title: str


# å“åº”æ¨¡å‹
class Paper(BaseModel):
    title: str
    authors: List[str]
    publishedDate: str
    source: str
    summary: str
    keywords: Optional[List[str]] = []
    url: Optional[str] = None
    pdfUrl: Optional[str] = None
    fullTextRetrieved: Optional[bool] = False


class SearchResult(BaseModel):
    papers: List[Paper]
    totalCount: int
    processingTime: float
    summary: Optional[str] = None
    actionPlan: Optional[List[str]] = None


# å…¨å±€å˜é‡
literature_agent = None


def get_agent():
    """è·å–æ–‡çŒ®ä»£ç†å®ä¾‹"""
    global literature_agent
    if literature_agent is None:
        try:
            print(">> æ­£åœ¨åˆå§‹åŒ–æ–‡çŒ®ä»£ç†...")
            # ä½¿ç”¨ç®€åŒ–çš„é…ç½®æ¥é¿å…é…ç½®è§£æé—®é¢˜
            from src.lit_review_agent.retrieval.arxiv_client import ArxivClient
            from src.lit_review_agent.retrieval.semantic_scholar_client import SemanticScholarClient

            # åˆ›å»ºç®€åŒ–çš„ä»£ç†å¯¹è±¡
            class SimpleLiteratureAgent:
                def __init__(self):
                    self.arxiv_client = ArxivClient(
                        api_url="http://export.arxiv.org/api/",
                        max_results=100
                    )

                    # ç®€åŒ–çš„é…ç½®å¯¹è±¡
                    class SimpleConfig:
                        def __init__(self):
                            self.semantic_scholar_api_key = None
                            self.semantic_scholar_timeout_seconds = 30
                            self.pdf_processing_timeout = 120
                            self.semantic_scholar_api_url = "https://api.semanticscholar.org/graph/v1"

                    self.semantic_scholar_client = SemanticScholarClient(
                        config=SimpleConfig())

                async def conduct_literature_review(self, **kwargs):
                    """ç®€åŒ–çš„æ–‡çŒ®ç»¼è¿°æ–¹æ³•"""
                    query = kwargs.get('raw_query') or kwargs.get(
                        'research_topic') or kwargs.get('query')
                    max_papers = kwargs.get('max_papers', 20)
                    sources = kwargs.get('sources', ['arxiv'])

                    print(f">> å¼€å§‹æœç´¢: {query}")

                    all_papers = []

                    # ä½¿ç”¨arXivæœç´¢
                    if 'arxiv' in sources:
                        try:
                            arxiv_results = await self.arxiv_client.search(
                                query=query,
                                max_results=min(max_papers, 10)
                            )
                            all_papers.extend(arxiv_results)
                            print(f">> ä»arXivè·å–åˆ° {len(arxiv_results)} ç¯‡è®ºæ–‡")
                        except Exception as e:
                            print(f">> arXivæœç´¢å¤±è´¥: {e}")

                    # ä½¿ç”¨Semantic Scholaræœç´¢
                    if 'semantic_scholar' in sources:
                        try:
                            s2_results = await self.semantic_scholar_client.search(
                                query=query,
                                max_results=min(max_papers, 10)
                            )
                            all_papers.extend(s2_results)
                            print(
                                f">> ä»Semantic Scholarè·å–åˆ° {len(s2_results)} ç¯‡è®ºæ–‡")
                        except Exception as e:
                            print(f">> Semantic Scholaræœç´¢å¤±è´¥: {e}")

                    # è½¬æ¢ä¸ºAPIæ ¼å¼
                    processed_papers = []
                    for paper in all_papers:
                        processed_papers.append({
                            'title': paper.title,
                            'authors': paper.authors,
                            'published_date': paper.publication_date.isoformat() if paper.publication_date else '',
                            'source': paper.source,
                            'summary': paper.abstract or '',
                            'keywords': [],
                            'url': paper.url,
                            'pdf_url': paper.pdf_url or '',
                            'full_text_retrieved': False
                        })

                    return {
                        'processed_papers': processed_papers,
                        'action_plan': [
                            f"ğŸ¯ ç¡®å®šç ”ç©¶ä¸»é¢˜ï¼š{query}",
                            "ğŸ“š é€‰æ‹©æ•°æ®æºï¼šarXiv",
                            f"ğŸ” æ‰§è¡Œæ£€ç´¢ç­–ç•¥ï¼šæ£€ç´¢æœ€å¤š{max_papers}ç¯‡ç›¸å…³è®ºæ–‡",
                            "ğŸ“Š åˆ†æè®ºæ–‡å…ƒæ•°æ®ï¼šæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ç­‰",
                            "ğŸ“ æ•´ç†æœç´¢ç»“æœ"
                        ]
                    }

            literature_agent = SimpleLiteratureAgent()
            print(">> ç®€åŒ–æ–‡çŒ®ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f">> ä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
            print(">> å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
            literature_agent = None
    return literature_agent


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "AI Literature Review API",
        "version": "3.1.0",
        "status": "running",
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        agent = get_agent()
        agent_status = "healthy" if agent else "degraded"
        return {
            "status": "ok",
            "timestamp": datetime.now().isoformat(),
            "agent_status": agent_status,
            "version": "3.1.0",
            "services": {
                "api": "running",
                "agent": agent_status,
                "database": "connected" if agent else "unavailable",
            },
        }
    except Exception as e:
        return {
            "status": "error",
            "timestamp": datetime.now().isoformat(),
            "error": str(e),
            "agent_status": "error",
        }


@app.get("/api/status")
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    agent = get_agent()
    return {
        "status": "healthy" if agent else "demo",
        "timestamp": datetime.now().isoformat(),
        "agent_initialized": agent is not None,
    }


@app.post("/api/search", response_model=SearchResult)
async def search_literature(request: SearchRequest):
    """æ–‡çŒ®æ£€ç´¢"""
    start_time = time.time()

    try:
        agent = get_agent()

        # Determine which query to use
        query_to_use = request.rawQuery or request.query
        if not query_to_use:
            raise HTTPException(
                status_code=400, detail="Either 'query' or 'rawQuery' must be provided"
            )

        print(f"å¼€å§‹æ£€ç´¢: {query_to_use}")

        if agent:
            # ä½¿ç”¨çœŸå®çš„ä»£ç†
            if request.rawQuery:
                # Use natural language processing
                results = await agent.conduct_literature_review(
                    raw_query=request.rawQuery,
                    max_papers=request.maxPapers,
                    sources=request.sources,
                    retrieve_full_text=request.retrieveFullText,
                    year_start=request.yearStart,
                    year_end=request.yearEnd,
                )
            else:
                # Use legacy structured query
                results = await agent.conduct_literature_review(
                    research_topic=request.query,
                    max_papers=request.maxPapers,
                    sources=request.sources,
                    retrieve_full_text=request.retrieveFullText,
                    year_start=request.yearStart,
                    year_end=request.yearEnd,
                )

            # è½¬æ¢ç»“æœæ ¼å¼
            papers = []
            action_plan = None

            if results and "processed_papers" in results:
                for paper_data in results["processed_papers"]:
                    paper = Paper(
                        title=paper_data.get("title", "æœªçŸ¥æ ‡é¢˜"),
                        authors=paper_data.get("authors", []),
                        publishedDate=paper_data.get("published_date", ""),
                        source=paper_data.get("source", "unknown"),
                        summary=paper_data.get(
                            "ai_enhanced_summary", paper_data.get(
                                "summary", "")
                        ),
                        keywords=paper_data.get("keywords", []),
                        url=paper_data.get("url", ""),
                        pdfUrl=paper_data.get("pdf_url", ""),
                        fullTextRetrieved=paper_data.get(
                            "full_text_retrieved", False),
                    )
                    papers.append(paper)

                # Extract action plan from results
                action_plan = results.get("action_plan", [])
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            papers = [
                Paper(
                    title=f"äººå·¥æ™ºèƒ½åœ¨{query_to_use}é¢†åŸŸçš„åº”ç”¨ç ”ç©¶",
                    authors=["å¼ ä¸‰", "æå››", "ç‹äº”"],
                    publishedDate="2024-01-15",
                    source="arxiv",
                    summary=f"æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨{query_to_use}é¢†åŸŸçš„æœ€æ–°åº”ç”¨ã€‚",
                    keywords=["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", query_to_use],
                    url="https://arxiv.org/abs/2401.12345",
                    fullTextRetrieved=True,
                ),
                Paper(
                    title=f"{query_to_use}ä¸­çš„æœºå™¨å­¦ä¹ æ–¹æ³•ç»¼è¿°",
                    authors=["èµµå…­", "é’±ä¸ƒ"],
                    publishedDate="2023-12-20",
                    source="semantic_scholar",
                    summary=f"æœ¬ç»¼è¿°åˆ†æäº†{query_to_use}é¢†åŸŸä¸­æœºå™¨å­¦ä¹ æ–¹æ³•çš„å‘å±•ç°çŠ¶ã€‚",
                    keywords=["æœºå™¨å­¦ä¹ ", "æ•°æ®åˆ†æ"],
                    url="https://example.com/paper2",
                    fullTextRetrieved=False,
                ),
            ]

            # ç”Ÿæˆæ¨¡æ‹Ÿçš„è¡ŒåŠ¨è®¡åˆ’
            action_plan = [
                f"ğŸ¯ ç¡®å®šç ”ç©¶ä¸»é¢˜ï¼š{query_to_use}",
                "ğŸ“š é€‰æ‹©æ•°æ®æºï¼šarxivã€semantic_scholar",
                f"ğŸ” æ‰§è¡Œæ£€ç´¢ç­–ç•¥ï¼šæ£€ç´¢æœ€å¤š{request.maxPapers}ç¯‡ç›¸å…³è®ºæ–‡",
                "ğŸ“Š åˆ†æè®ºæ–‡å…ƒæ•°æ®ï¼šæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€å¼•ç”¨æ•°ç­‰",
                "ğŸ“ˆ è¯†åˆ«ç ”ç©¶è¶‹åŠ¿ï¼šå‘è¡¨æ—¶é—´åˆ†å¸ƒã€çƒ­ç‚¹å…³é”®è¯",
                "ğŸ¤– AIæ™ºèƒ½åˆ†æï¼šç”Ÿæˆç»¼åˆæ€§ç ”ç©¶æ´å¯Ÿ",
                "ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼šæ•´ç†å‘ç°å’Œå»ºè®®",
            ]

        processing_time = time.time() - start_time

        print(f"æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œè€—æ—¶ {processing_time:.2f}s")

        return SearchResult(
            papers=papers,
            totalCount=len(papers),
            processingTime=processing_time,
            summary=f"åŸºäº'{query_to_use}'çš„æ–‡çŒ®æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ°{len(papers)}ç¯‡ç›¸å…³è®ºæ–‡ã€‚",
            actionPlan=action_plan,
        )

    except Exception as e:
        print(f"æ£€ç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ£€ç´¢å¤±è´¥: {str(e)}")


@app.post("/api/generate-report")
async def generate_report(request: ReportRequest):
    """ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
    try:
        print(f"å¼€å§‹ç”ŸæˆæŠ¥å‘Š: {request.title}")

        # æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆ
        await asyncio.sleep(2)

        report = f"""# {request.title}

## æ‘˜è¦

æœ¬æŠ¥å‘ŠåŸºäº {len(request.papers)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œå¯¹ç ”ç©¶ä¸»é¢˜è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°åˆ†æã€‚

## ä¸»è¦å‘ç°

### 1. ç ”ç©¶ç°çŠ¶
- å…±æ£€ç´¢åˆ° {len(request.papers)} ç¯‡é«˜è´¨é‡ç›¸å…³æ–‡çŒ®
- ç ”ç©¶æ¶µç›–äº†å¤šä¸ªé‡è¦çš„æŠ€æœ¯æ–¹å‘å’Œåº”ç”¨åœºæ™¯

### 2. æŠ€æœ¯è¶‹åŠ¿
- äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨
- è·¨å­¦ç§‘èåˆæˆä¸ºé‡è¦å‘å±•æ–¹å‘

## ç»“è®º

é€šè¿‡å¯¹ç°æœ‰æ–‡çŒ®çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å‘ç°è¯¥é¢†åŸŸæ­£å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µã€‚

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*
        """

        print("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")

        return {"report": report}

    except Exception as e:
        print(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    import uvicorn

    print("å¯åŠ¨ FastAPI æœåŠ¡å™¨...")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False, log_level="info")
